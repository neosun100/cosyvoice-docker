# =============================================================================
# CosyVoice TTS Server - Python Dependencies
# =============================================================================
# Core dependencies for TTS inference.
# vLLM is optional - see comments below for installation.
# =============================================================================
--extra-index-url https://download.pytorch.org/whl/cu121
--extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/ # https://github.com/microsoft/onnxruntime/issues/21684

# -----------------------------------------------------------------------------
# PyTorch (CUDA 12.1)
# -----------------------------------------------------------------------------
torch==2.3.1
torchaudio==2.3.1

# -----------------------------------------------------------------------------
# Core ML/Audio Libraries
# -----------------------------------------------------------------------------
transformers==4.51.3
numpy==1.26.4
librosa==0.10.2
soundfile==0.12.1

# -----------------------------------------------------------------------------
# CosyVoice Dependencies
# -----------------------------------------------------------------------------
openai-whisper==20240930
tiktoken==0.8.0
conformer==0.3.2
diffusers==0.29.0
hydra-core==1.3.2
HyperPyYAML==1.2.3
ruamel.yaml<0.19.0
inflect==7.3.1
omegaconf==2.3.0
modelscope==1.20.0
wetext==0.0.4
x-transformers==2.11.24

# Matcha-TTS Runtime Dependencies
lightning==2.2.4
gdown==5.1.0
wget==3.2

# -----------------------------------------------------------------------------
# API Server
# -----------------------------------------------------------------------------
fastapi==0.115.6
uvicorn==0.30.0
python-multipart==0.0.9
pydantic==2.7.0

# -----------------------------------------------------------------------------
# Utilities
# -----------------------------------------------------------------------------
rich==13.7.1
networkx==3.1
matplotlib==3.7.5
pyarrow==18.1.0
pyworld==0.3.4

# -----------------------------------------------------------------------------
# ONNX Runtime (for speech tokenizer)
# -----------------------------------------------------------------------------
onnx==1.16.0
onnxruntime-gpu==1.18.0; sys_platform == 'linux'
onnxruntime==1.18.0; sys_platform == 'darwin' or sys_platform == 'win32'

# -----------------------------------------------------------------------------
# Optional: vLLM Acceleration
# -----------------------------------------------------------------------------
# vLLM is NOT included by default due to:
#   1. Strict GPU/driver requirements (not all hardware supports it)
#   2. Potential dependency conflicts
#   3. Version sensitivity (only 0.9.0 or 0.11.x+ supported)
#
# To enable vLLM, use the vLLM Docker image or install manually:
#
#   # For vLLM 0.11.x (recommended, requires transformers upgrade)
#   pip install vllm==0.11.0 transformers==4.57.1
#
#   # For vLLM 0.9.0 (legacy, compatible with current transformers)
#   pip install vllm==0.9.0
#
# Then set ENABLE_VLLM=true when running the container.
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# Optional: TensorRT Acceleration
# Uncomment if using load_trt=True (requires compatible GPU):
# -----------------------------------------------------------------------------
# --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/
# tensorrt-cu12==10.13.3.9; sys_platform == 'linux'
# tensorrt-cu12-bindings==10.13.3.9; sys_platform == 'linux'
# tensorrt-cu12-libs==10.13.3.9; sys_platform == 'linux'

# -----------------------------------------------------------------------------
# Optional: Training Dependencies
# Uncomment if training models:
# -----------------------------------------------------------------------------
# deepspeed==0.15.1; sys_platform == 'linux'
# lightning==2.2.4
# tensorboard==2.14.0
